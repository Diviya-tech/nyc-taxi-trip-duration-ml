# ğŸš– NYC Taxi Trip Duration Prediction

![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![TensorFlow](https://img.shields.io/badge/TensorFlow-Deep%20Learning-orange?logo=tensorflow)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)
![License](https://img.shields.io/badge/License-MIT-green)

Predicting NYC taxi trip durations using neural networks â€” comparing three architectures of increasing depth and complexity, built from feature engineering to final evaluation.

---

## ğŸ¯ What This Project Does

Given pickup/dropoff location, time, passenger count and other trip features, predict how long a taxi ride will take. Simple problem on the surface â€” but the data is messy, the feature engineering matters a lot, and the model architecture choices make a real difference.

---

## ğŸ“Š Dataset

**Source:** [NYC Taxi Trip Duration â€“ Kaggle](https://www.kaggle.com/competitions/nyc-taxi-trip-duration)

- ~1.4 million taxi trips in New York City
- Features: pickup/dropoff coordinates, timestamps, passenger count, vendor ID
- Target: trip duration in seconds (log-transformed for training)

---

## ğŸ” What I Built

### Neural Network from Scratch (XOR Proof of Concept)
Before training on the taxi data, I built and validated a neural network from scratch using NumPy on the XOR problem â€” testing sigmoid vs tanh activations to understand convergence behavior before scaling up.

### 3 Neural Network Architectures (Keras/TensorFlow)

| Model | Architecture | Learning Rate | Activation |
|-------|-------------|---------------|------------|
| Model_1 | 64 | 0.003 | tanh |
| Model_2 | 128 â†’ 64 | 0.001 | relu |
| Model_3 | 256 â†’ 128 â†’ 64 | 0.0005 | relu |

All models trained with early stopping, StandardScaler normalization, and 80/10/10 train/val/test split.

---

## ğŸ“ˆ Results

### Model Performance Comparison

| Model | MSE | MAE | RÂ² |
|-------|-----|-----|----|
| Model_1 (64, tanh) | 0.5070 | 0.5258 | 0.2059 |
| Model_2 (128â†’64, relu) | 0.3585 | 0.4264 | 0.4385 |
| **Model_3 (256â†’128â†’64, relu)** | **0.3581** | **0.4269** | **0.4392** |

> Model_3 wins â€” deeper architecture with smaller learning rate converged best. Model_2 and Model_3 are very close, suggesting diminishing returns beyond 2 layers for this dataset.

---

### XOR Training Loss â€” Sigmoid vs Tanh

![XOR Training Loss](results/xor_training_loss.png)

> tanh converged ~3x faster than sigmoid on XOR â€” which informed the activation function choices for the taxi models.

---

### Model_3 â€” Train vs Validation Loss

![Model 3 Train Val Loss](results/model_3_train_vs_loss_best.png)
> Clean convergence with no overfitting â€” train and val loss track closely throughout training.

---

### Validation Loss Comparison Across All Models

![Validation Loss Comparison](results/validation_across_all_models.png)

> Model_1 (tanh, single layer) plateaus early around 0.51 MSE. Model_2 and Model_3 both drop to ~0.36, confirming that deeper architectures with relu handle this regression task better.

---

## ğŸ’¬ My Notes

The XOR experiment before jumping into the main model was actually really useful â€” it made me understand WHY tanh and sigmoid behave differently before I had to care about it on a larger dataset. Model_2 and Model_3 ended up almost identical in performance which was surprising â€” I expected the deeper model to win more clearly.

---

## ğŸ› ï¸ Tech Stack

`Python` `TensorFlow` `Keras` `NumPy` `Pandas` `Scikit-learn` `Matplotlib` `Jupyter Notebook`

---

## ğŸš€ How to Run

```bash
git clone https://github.com/Diviya-tech/nyc-taxi-trip-duration-ml
cd nyc-taxi-trip-duration-ml
pip install -r requirements.txt
jupyter notebook
```

---

## ğŸ“ Project Structure

```
nyc-taxi-trip-duration-ml/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ xor_training_loss.png
â”‚   â”œâ”€â”€ model_3_train_vs_loss_best.png
â”‚   â””â”€â”€ validation_across_all_models.png
â”œâ”€â”€ notebooks/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ”® What's Next

- Try gradient boosting (XGBoost/LightGBM) â€” likely to outperform neural nets on tabular data
- Add distance feature using Haversine formula between pickup/dropoff coordinates
- Experiment with time-based features (rush hour, day of week)

---

## ğŸ“¬ Connect

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat&logo=linkedin)](https://linkedin.com/in/sridivyadasari)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black?style=flat&logo=github)](https://github.com/Diviya-tech)
